{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FaceGuard FaceEmbedding Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following training approach utilizes triplet loss to train a faceembedding model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import os   \n",
    "from os.path import exists, join\n",
    "import numpy as np\n",
    "from PIL import Image  \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg \n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "from torchvision import datasets, models, transforms  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Configuration \n",
    "Please set the variable to the downloaded folder set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dataset_folder = 'dataset/lfw/lfw'\n",
    "\n",
    "#Place where the dataset split copy should be placed\n",
    "var_dataset_split_folder = 'dataset/lfw/triplet_dataset'\n",
    "\n",
    "var_val_split = 0.2\n",
    "var_test_split = 0.1\n",
    "var_batch_size = 8 \n",
    "\n",
    "var_mtcnn_image_size = 160\n",
    "var_mtcnn_margin = 0 \n",
    "\n",
    "if not os.path.exists(var_dataset_split_folder ):\n",
    "    os.makedirs(var_dataset_split_folder )\n",
    "if not os.path.exists(var_dataset_split_folder  + '/train'):\n",
    "    os.makedirs(var_dataset_split_folder +'/train')   \n",
    "if not os.path.exists(var_dataset_split_folder  + '/validation'):\n",
    "    os.makedirs(var_dataset_split_folder +'/validation') \n",
    "if not os.path.exists(var_dataset_split_folder  + '/test'):\n",
    "    os.makedirs(var_dataset_split_folder +'/test')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset \n",
    "\n",
    "The following steps prepare the dataset by first calculating the mean and standard deviation and then generate triplets files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = f\"python utils/calculate_rgb_mean_std.py\\\n",
    "                    --dir {var_dataset_folder}\"\n",
    "!{run}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and Standard dev \n",
    "mean = [0.6158, 0.4637, 0.3757]\n",
    "std = [0.2124, 0.1863, 0.1812] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split persons in Train, Val and Test set by copying training data \n",
    "import shutil   \n",
    "\n",
    "names = os.listdir(var_dataset_folder)   \n",
    "n_samples = len(names) \n",
    " \n",
    "shuffled_indices = np.random.permutation(n_samples)\n",
    "testset_inds = shuffled_indices[:int(n_samples * var_test_split)]\n",
    "validationset_inds = shuffled_indices[int(n_samples * var_test_split) : int(n_samples * var_test_split) + int(n_samples * var_val_split)]\n",
    "trainingset_inds = shuffled_indices[int(n_samples * var_test_split) + int(n_samples * var_val_split) :] \n",
    "\n",
    "print(len(testset_inds))\n",
    "print(len(validationset_inds))\n",
    "print(len(trainingset_inds))\n",
    "print(names[testset_inds[0]])\n",
    "\n",
    "for i in testset_inds: \n",
    "    shutil.copytree(f'{var_dataset_folder}/{names[i]}', f'{var_dataset_split_folder}/test/{names[i]}') \n",
    "for i in trainingset_inds: \n",
    "    shutil.copytree(f'{var_dataset_folder}/{names[i]}', f'{var_dataset_split_folder }/train/{names[i]}') \n",
    "for i in validationset_inds: \n",
    "    shutil.copytree(f'{var_dataset_folder}/{names[i]}', f'{var_dataset_split_folder }/validation/{names[i]}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class TripletGenerator: \n",
    "    def __init__(self, input_dir, output_file):  \n",
    "        if not os.path.exists(input_dir): \n",
    "            raise Exception('Input dir does not exist!') \n",
    "        \n",
    "        self.input_dir = input_dir \n",
    "            \n",
    "        self.output_file = output_file\n",
    "    \n",
    "    def generate_triplets(self):    \n",
    "        names = os.listdir(self.input_dir) \n",
    "    \n",
    "        for name in names:  \n",
    "            for file in os.listdir(self.input_dir + \"/\" + name):  \n",
    "                in_filepath = '{}/{}/{}'.format(self.input_dir, name, file) \n",
    "                \n",
    "                # TODO: Positive \n",
    "                positivearray = os.listdir(self.input_dir + \"/\" + name) \n",
    "                if len(positivearray) == 1: \n",
    "                    positive_file = positivearray[0] \n",
    "                else: \n",
    "                    idx_file = positivearray.index(file) \n",
    "                    del positivearray[idx_file] \n",
    "                    positive_file = random.choice(positivearray)\n",
    "                \n",
    "                #Negative    \n",
    "                # First remove own name from list \n",
    "                namescopy = names.copy(); \n",
    "                idx = namescopy.index(name)  \n",
    "                del namescopy[idx]\n",
    "                \n",
    "                random_name = random.choice(namescopy) \n",
    "                random_file = random.choice(os.listdir('{}/{}'.format(self.input_dir, random_name))) \n",
    "                \n",
    "                print('anchor: {}, positive: {}, negative: {}'.format(file, positive_file, random_file)) \n",
    "                \n",
    "                with open(self.output_file, 'a') as f:\n",
    "                    f.write('{}\\t{}\\t{}\\n'.format(file, positive_file, random_file))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgen_train = TripletGenerator(f'{var_dataset_split_folder}/train', 'triplets-train.txt') \n",
    "tgen_train.generate_triplets() \n",
    "tgen_test = TripletGenerator(f'{var_dataset_split_folder}/test', 'triplets-test.txt') \n",
    "tgen_test.generate_triplets() \n",
    "tgen_val = TripletGenerator(f'{var_dataset_split_folder}/validation', 'triplets-validation.txt') \n",
    "tgen_val.generate_triplets() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset  \n",
    "import os \n",
    "\n",
    "class LFWDataset(Dataset): \n",
    "    \n",
    "    def __init__(self, triplet_file, path): \n",
    "        self.triplets_file = triplet_file\n",
    "        self.read_triplets()  \n",
    "        self.path = path\n",
    "        \n",
    "    def read_triplets(self): \n",
    "        triplet_rows = open(self.triplets_file).read().splitlines()  \n",
    "        self.triplets = [] \n",
    "        for triplet in triplet_rows: \n",
    "            split = triplet.split('\\t')\n",
    "            self.triplets.append({\"anchor\": split[0], \"positive\": split[1], \"negative\": split[2]})\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.triplets) \n",
    "    \n",
    "    def person_from_path(self, path): \n",
    "        path = path.replace('augmented_', '').replace('bright_', '').replace('dark_', '').replace('hflip_', '').replace('mask_', '')\n",
    "        type = path.split('.') \n",
    "        comp = type[0].split('_')\n",
    "        \n",
    "        o = \"_\" \n",
    "        return o.join(comp[:-1]) \n",
    "    \n",
    "    def preprocess_image(self, path): \n",
    "        path = '{}{}/{}'.format(self.path, self.person_from_path(path), path) \n",
    "        ex_img = Image.open(path)  \n",
    "        \n",
    "        transform = transforms.Compose(\n",
    "            [transforms.ToTensor(),\n",
    "             transforms.Normalize(mean, std)]\n",
    "        )\n",
    "        image = transform(ex_img) \n",
    "        return image.cuda()\n",
    "        \n",
    "    def __getitem__(self, idx):  \n",
    "        return self.preprocess_image(self.triplets[idx]['anchor']), self.preprocess_image(self.triplets[idx]['positive']), self.preprocess_image(self.triplets[idx]['negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfwdataset_train = LFWDataset('triplets-train.txt', f'{var_dataset_split_folder}/train/') \n",
    "lfwdataset_test = LFWDataset('triplets-test.txt', f'{var_dataset_split_folder}/test/')  \n",
    "lfwdataset_val = LFWDataset('triplets-validation.txt', f'{var_dataset_split_folder}/validation/') \n",
    "\n",
    "print(f'Train: {len(lfwdataset_train)}') \n",
    "print(f'Val: {len(lfwdataset_val)}') \n",
    "print(f'Test: {len(lfwdataset_test)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=lfwdataset_train,\n",
    "                                           batch_size=var_batch_size,\n",
    "                                           num_workers=0,\n",
    "                                           shuffle=True, sampler=None,\n",
    "                                           collate_fn=None)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=lfwdataset_test,\n",
    "                                           batch_size=var_batch_size,\n",
    "                                           num_workers=0,\n",
    "                                           shuffle=False, sampler=None,\n",
    "                                           collate_fn=None)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=lfwdataset_val,\n",
    "                                           batch_size=var_batch_size,\n",
    "                                           num_workers=0,\n",
    "                                           shuffle=False, sampler=None,\n",
    "                                           collate_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model  \n",
    "Easily setup your model by choosing the right import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models   \n",
    "import torch.nn as nn\n",
    "#from facenet_pytorch import InceptionResnetV1\n",
    "#from torchvision.models.inception_resnet_v1 import InceptionResnetV1\n",
    "    \n",
    "model = models.resnet34(pretrained=False)\n",
    "model.fc = nn.Linear(512, 512) \n",
    "model = model.cuda() \n",
    "\n",
    "#model = models.resnet50(pretrained=False)   \n",
    "#model = models.resnet18(pretrained=False) \n",
    "\n",
    "#from inception_resnet_v2 import Inception_ResNetv2\n",
    "#model = Inception_ResNetv2()\n",
    "#model = model.cuda() \n",
    "\n",
    "#model = InceptionResnetV1(pretrained='vggface2').eval().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_funct = nn.TripletMarginLoss(margin=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime  \n",
    "import os\n",
    "\n",
    "class TensorBoardLogger: \n",
    "    \n",
    "    def __init__(self, runname, date=datetime.today().strftime('%Y-%m-%d')):   \n",
    "        path = 'logs/{}/{}'.format(date, runname)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path) \n",
    "        self.writer = SummaryWriter(path) \n",
    "    \n",
    "    def log_loss(self, mode, loss, epoch): \n",
    "        self.writer.add_scalar(f\"loss/{mode}\", loss, epoch)  \n",
    "        self.writer.close()\n",
    "    \n",
    "    def log_acc(self, mode, acc, epoch): \n",
    "        self.writer.add_scalar(f\"acc/{mode}\", acc, epoch) \n",
    "        self.writer.close()\n",
    "        \n",
    "    def log_threshold_for_acc(self, mode, threshold, epoch): \n",
    "        self.writer.add_scalar(f\"threshold/{mode}\", threshold, epoch) \n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingSpace:  \n",
    "    \n",
    "    \"\"\"  \n",
    "    :param model: The model that creates the image embeddings \n",
    "    :param tag: is the name the space will be stored in tensorboard \n",
    "    :param input_dir: directory where the images are stored that shall be loaded \n",
    "    :param number: Number of persons to consider - 10 persons with 10 images = 100 dots in the space  \n",
    "    :param tensorboardlogger: The tensorboard writer created for this run \n",
    "    \"\"\"\n",
    "    def __init__(self, model, tag, input_dir, number, tensorboardlogger): \n",
    "        self.model = model \n",
    "        self.tag = tag \n",
    "        self.input_dir = input_dir \n",
    "        self.number = number  \n",
    "        self.tensorboardlogger = tensorboardlogger\n",
    "    \n",
    "    \"\"\" \n",
    "    :param path: Image that is used to create a new embedding  \n",
    "    \"\"\"\n",
    "    def calc_embedding(self, path):  \n",
    "        image = Image.open(path) \n",
    "        transformed_image = transforms.ToTensor()(image).cuda() \n",
    "        return self.model.forward(transformed_image.unsqueeze(dim=0))   \n",
    "    \n",
    "    \"\"\" \n",
    "    Create the embedding space and add to tensorboard \n",
    "    \"\"\"\n",
    "    def create_space(self): \n",
    "        embeddings = [] \n",
    "        labels = []  \n",
    "        \n",
    "        persons = os.listdir(self.input_dir)[:self.number]  \n",
    "        for name in persons:   \n",
    "            for file in os.listdir(\"{}/{}\".format(self.input_dir, name)):  \n",
    "                in_filepath = '{}/{}/{}'.format(self.input_dir, name, file)  \n",
    "                embeddings.append(self.calc_embedding(in_filepath)) \n",
    "                labels.append(name) \n",
    "        \n",
    "        # Concat all embeddings into one tensor along axis 0 \n",
    "        embedding_obj = torch.cat(embeddings, 0)\n",
    "        \n",
    "        self.tensorboardlogger.writer.add_embedding(embedding_obj, tag=self.tag, metadata=labels) \n",
    "        self.tensorboardlogger.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Performance measurements\n",
    "\"\"\"  \n",
    "\n",
    "class PerformanceMeasurement: \n",
    "    \n",
    "    def __init__(self): \n",
    "        self.predictions = {}  \n",
    "        self.thresholds = np.arange(0, 40, 0.1)   \n",
    "    \n",
    "    \"\"\" \n",
    "    Prediction inspired by faceguard pipeline \n",
    "    We calculate the distance between the anchor embedding and a \n",
    "    comparison embedding\n",
    "\n",
    "    If the distance is larger than a threshold, we return that the \n",
    "    images are not the same (0) otherwise they are the same (1) \n",
    "    \n",
    "    :param anchor_embedding: Embedding vector of the anchor \n",
    "    :param compare_embedding: Embedding vector to compare with the anchor \n",
    "    :param threshold: Highest distance until anchor and compare are the same  \n",
    "    \"\"\"\n",
    "    def prediction(self, anchor_embedding, compare_embedding, threshold):  \n",
    "        dist = np.linalg.norm(np.subtract(anchor_embedding.cpu().detach().numpy(), compare_embedding.cpu().detach().numpy()))   \n",
    "        if dist > threshold: \n",
    "            return 0 \n",
    "        else: \n",
    "            return 1 \n",
    "    \n",
    "    \"\"\" \n",
    "    Add new training run \n",
    "    :param anchor_embedding: Embedding vector of the anchor \n",
    "    :param anchor_embedding: Embedding vector of the same person as anchor  \n",
    "    :param negative_embedding: Negative example \n",
    "    \"\"\"\n",
    "    def add_new_run(self, anchor_embedding, positive_embedding, negative_embedding):  \n",
    "        for threshold in self.thresholds: \n",
    "            #Create entry if it does not exist\n",
    "            if threshold not in self.predictions: \n",
    "                self.predictions[threshold] = {}   \n",
    "                self.predictions[threshold]['tp'] = 0 \n",
    "                self.predictions[threshold]['fp'] = 0  \n",
    "                self.predictions[threshold]['tn'] = 0  \n",
    "                self.predictions[threshold]['fn'] = 0 \n",
    "\n",
    "            # The positive prediction (so comparison between anchor and positive) should return 1\n",
    "            pred_pos = self.prediction(anchor_embedding, positive_embedding, threshold)  \n",
    "            \n",
    "            # The negative prediction (so comparison between anchor and negative) should return 0\n",
    "            pred_neg = self.prediction(anchor_embedding, negative_embedding, threshold) \n",
    "\n",
    "            if pred_neg == 0: \n",
    "                self.predictions[threshold]['tn'] += 1  \n",
    "            else: \n",
    "                self.predictions[threshold]['fp'] += 1 \n",
    "\n",
    "            if pred_pos == 1: \n",
    "                self.predictions[threshold]['tp'] += 1 \n",
    "            else: \n",
    "                self.predictions[threshold]['fn'] += 1  \n",
    "    \n",
    "    \"\"\" \n",
    "    Simple accuracy helper function \n",
    "    \"\"\"\n",
    "    def calc_acc(self, tp, tn, fp, fn):  \n",
    "        return (tp+tn)/(tp+tn+fp+fn) \n",
    "    \n",
    "    \n",
    "    \"\"\" \n",
    "    Function that gets called after all runs \n",
    "    Here we calculate the accuracy for all thresholds and return the highest accuracy \n",
    "    \n",
    "    This also helps us set the threshold for the pipeline function \n",
    "    \"\"\"\n",
    "    def calc_total_acc(self):  \n",
    "        max_acc = 0  \n",
    "        max_t = 0  \n",
    "        \n",
    "        for threshold in self.predictions: \n",
    "            acc = self.calc_acc(self.predictions[threshold]['tp'], self.predictions[threshold]['tn'], self.predictions[threshold]['fp'], self.predictions[threshold]['fn']) \n",
    "            if acc > max_acc:  \n",
    "                print(f\"Trehshold {threshold} > Acc: {acc}\") \n",
    "                max_acc = acc \n",
    "                max_t = threshold\n",
    "        \n",
    "        return max_acc, max_t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  \n",
    "\n",
    "class Trainer: \n",
    "    def __init__(self, run_tag, model, train_loader, valid_loader, mean, std, optimizer, loss_funct, measure_performance_of_epochs, create_embedding_space=False): \n",
    "        \n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader  \n",
    "        self.tag = run_tag\n",
    "         \n",
    "        self.mean = mean \n",
    "        self.std = std \n",
    "        \n",
    "        self.logger = TensorBoardLogger(run_tag) \n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_func = loss_funct\n",
    "        \n",
    "        self.train_loss = [] \n",
    "        self.validation_loss = []\n",
    "        self.acc = [] \n",
    "        \n",
    "        self.measure_performance = measure_performance_of_epochs\n",
    "        \n",
    "        # if the optimizer is not initialzed \n",
    "        if optimizer:\n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            self.optimizer = torch.optim.Adam(**optimizer_args)\n",
    "\n",
    "        \n",
    "    def train_epoch(self): \n",
    "        total_loss = 0  \n",
    "        \n",
    "        perf_measure = PerformanceMeasurement() \n",
    "        \n",
    "        for i, data in enumerate(self.train_loader):  \n",
    "            anchor, positive, negative = data  \n",
    "        \n",
    "            anchor_embedding = self.model.forward(anchor)  \n",
    "            positive_embedding = self.model.forward(positive) \n",
    "            negative_embedding = self.model.forward(negative)  \n",
    "            \n",
    "            self.optimizer.zero_grad() \n",
    "             \n",
    "            loss = loss_funct(anchor_embedding, positive_embedding, negative_embedding)\n",
    "            total_loss += loss.item() \n",
    "            loss.backward()            \n",
    "            \n",
    "            if self.measure_performance: \n",
    "                for embedding in range(len(anchor_embedding)): \n",
    "                    perf_measure.add_new_run(anchor_embedding[embedding], positive_embedding[embedding], negative_embedding[embedding]) \n",
    "        \n",
    "\n",
    "            self.optimizer.step() \n",
    "            if i % 40 == 0: \n",
    "                print(\"Batchno {} - {}%\".format(i, ((i / len(self.train_loader))*100))) \n",
    "        \n",
    "        return (total_loss) / len(self.train_loader), perf_measure.calc_total_acc() \n",
    "    \n",
    "    def train(self, n_epochs, start_epoch=0): \n",
    "        print(f\"Starting to train {n_epochs} epochs\")\n",
    "        for e in range(n_epochs): \n",
    "            print(\"Starting epoch {} with {} batches\".format(e+start_epoch, len(self.train_loader)))\n",
    "            \n",
    "            loss, trainacc = self.train_epoch()  \n",
    "            self.logger.log_loss('train', loss, e+start_epoch)  \n",
    "            self.logger.log_acc('train', trainacc[0], e+start_epoch) \n",
    "            self.logger.log_threshold_for_acc('train', trainacc[1], e+start_epoch)\n",
    "            \n",
    "            self.train_loss.append(loss) \n",
    "            \n",
    "            vallos, valacc = self.evaluate()   \n",
    "            self.validation_loss.append(vallos) \n",
    "            self.logger.log_loss('validation', vallos, e+start_epoch)  \n",
    "            self.logger.log_acc('validation', valacc[0], e+start_epoch) \n",
    "            self.logger.log_threshold_for_acc('validation', valacc[1], e+start_epoch)\n",
    "            \n",
    "            print(f\"Epoch {e+start_epoch} completetd with trainloss {loss}, trainacc {trainacc} and validationloss: {vallos}, validationacc {valacc}\") \n",
    "            self.plotloss() \n",
    "            \n",
    "            if create_embedding_space == True: \n",
    "                embeddingspace = EmbeddingSpace(self.model, f\"{self.tag}-epoch-{e}\", 'dataset/lfw/lfw_cropeed', 10, self.logger) \n",
    "                embeddingspace.create_space()  \n",
    "                print(\"Created embedding space for epoch.\")\n",
    "    \n",
    "    def evaluate(self):   \n",
    "        print(\"Starting evalutaion\")\n",
    "        eval_total_loss = 0  \n",
    "        perf_measure = PerformanceMeasurement() \n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            for i, data in enumerate(self.train_loader):  \n",
    "                anchor, positive, negative = data  \n",
    "\n",
    "                #print(\"{} - Training {} \".format(i, anchor))\n",
    "                anchor_embedding = self.model.forward(anchor)  \n",
    "                positive_embedding = self.model.forward(positive) \n",
    "                negative_embedding = self.model.forward(negative)  \n",
    "                \n",
    "                if self.measure_performance: \n",
    "                    for embedding in range(len(anchor_embedding)): \n",
    "                        perf_measure.add_new_run(anchor_embedding[embedding], positive_embedding[embedding], negative_embedding[embedding]) \n",
    "                \n",
    "                loss = loss_funct(anchor_embedding, positive_embedding, negative_embedding)\n",
    "                eval_total_loss += loss.item()   \n",
    "                \n",
    "        return (eval_total_loss) / len(self.valid_loader), perf_measure.calc_total_acc() \n",
    "    \n",
    "    def plotloss(self): \n",
    "        fig, axs = plt.subplots(2)  \n",
    "        axs[0].plot(np.array(self.train_loss)) \n",
    "        axs[0].set_title('Training loss')  \n",
    "        axs[1].plot(np.array(self.validation_loss)) \n",
    "        axs[1].set_title('Validation loss')  \n",
    "        fig.tight_layout(pad=2.0)\n",
    "        plt.show() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer('resnet-34-tripletloss-run-1-batchsize-8', model, train_loader, val_loader, mean, std, optimizer, loss_funct, False, False) \n",
    "trainer.train(30, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'inception-resnet-v2-no-augmentation-tripletloss-new-split-run-1-wmeanandstd-batchsize-8.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test():  \n",
    "    perf_measure_test = PerformanceMeasurement()   \n",
    "    print(\"Starting test\")\n",
    "    for i, data in enumerate(test_loader):   \n",
    "        anchor, positive, negative = data  \n",
    "\n",
    "        anchor_embedding = model.forward(anchor)  \n",
    "        positive_embedding = model.forward(positive) \n",
    "        negative_embedding = model.forward(negative)  \n",
    "                \n",
    "        for embedding in range(len(anchor_embedding)): \n",
    "            perf_measure_test.add_new_run(anchor_embedding[embedding], positive_embedding[embedding], negative_embedding[embedding]) \n",
    "    \n",
    "    if i % 40 == 0: \n",
    "        print(\"Batchno {} - {}%\".format(i, ((i / len(self.test_loader))*100)))  \n",
    "    \n",
    "    return perf_measure_test.calc_total_acc() \n",
    "\n",
    "logger = TensorBoardLogger('inception-resnet-v1-pretrained') \n",
    "acc = run_test()  \n",
    "print(acc)\n",
    "logger.log_acc('test', acc[0], 0) \n",
    "logger.log_threshold_for_acc('test', acc[1], 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:opence] *",
   "language": "python",
   "name": "conda-env-opence-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
