{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FaceGuard Classification  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following experiments make use of a faceembedding model to generate faceembeddings for over 150 persons and compare the classification speed of a loop comparison approach, the annoy nearest neightbor prediction as well as a support vector machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import os \n",
    "import numpy as np \n",
    "from PIL import Image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Facedetection Model for facecropping and bounding box prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "class FaceDetection:  \n",
    "\t\n",
    "\tmtcnn = MTCNN(image_size=160,margin=0) \n",
    "\t\n",
    "\tdef __init__(self): \n",
    "\t\tpass \n",
    "\t\n",
    "\tdef cropface(self, img): \n",
    "\t\treturn self.mtcnn(img)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "facedetection_model = FaceDetection() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 FaceEmbedding model \n",
    "We use a pretrained InceptionResnetV1 for these experiments. You can freely choose to include your own trained model here or use one of ours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import InceptionResnetV1\n",
    "from torchvision import transforms\n",
    "import numpy as np \n",
    "import torchvision.models as models   \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class EmbeddingModel:\n",
    "\n",
    "    def __init__(self): \n",
    "        self.model = InceptionResnetV1(pretrained='vggface2').eval().cuda()\n",
    "        #self.model = models.resnet34(pretrained=False) \n",
    "        #self.model = models.resnet34(pretrained=False)\n",
    "        #self.model.fc = nn.Linear(512, 512) \n",
    "\n",
    "        #self.model.load_state_dict(torch.load('./resnet-34-no-augmentation-tripletloss-new-split-run-1-wmeanandstd-batchsize-8-outsize-512.pth'))\n",
    "        \n",
    "        #self.model.eval()  \n",
    "        #self.model.cuda()\n",
    "\n",
    "    def generateEmbedding(self, img):\n",
    "        #img_embedding = self.resnet(img.unsqueeze(0)) \n",
    "        img_embedding = self.model.forward(img.unsqueeze(0))  \n",
    "\n",
    "        return img_embedding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = EmbeddingModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the first image from our training dataset into an array and already create the embeeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../faceembedding/lfw_cropped'  \n",
    "testsize = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = os.listdir(input_dir)[:testsize] \n",
    "\n",
    "train_images = [] \n",
    "for name in names:\n",
    "    for file in os.listdir(input_dir + \"/\" + name)[:1]:\n",
    "        train_images.append(input_dir+\"/\"+name+\"/\"+file) \n",
    "\n",
    "embeddings = [] \n",
    "for trainimage in train_images: \n",
    "    image = Image.open(trainimage)  \n",
    "    image = transforms.ToTensor()(image)\n",
    "    embedding = embedding_model.generateEmbedding(image.cuda())   \n",
    "    embeddings.append(embedding.cpu())  \n",
    "    del embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose one image from our training data that is unseen and generate an embedding vector. This is an example for a newly taken image that is used for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Astrid_Eyzaguirre\n"
     ]
    }
   ],
   "source": [
    "print(names[0])\n",
    "imgname = os.listdir(input_dir + \"/\" + names[0])[1] \n",
    "image = Image.open(input_dir + \"/\" + names[0]+\"/\"+imgname)   \n",
    "image = transforms.ToTensor()(image)\n",
    "embeddingtest = embedding_model.generateEmbedding(image.cuda())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Memory Database approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class Database:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.database={}\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(list(self.database.keys()))\n",
    "\n",
    "    def exists(self, name):\n",
    "        return name in self.database\n",
    "\n",
    "    def add_to_db(self, name, embedding):\n",
    "        if self.exists(name):\n",
    "            self.database[name].append(embedding)\n",
    "        else:\n",
    "            self.database[name] = [embedding]\n",
    "\n",
    "    def delete_from_db(self, name):\n",
    "        if self.exists(name):\n",
    "            self.database[name] = None\n",
    "        else:\n",
    "            raise Exception('Name does not exist in database')\n",
    "\n",
    "    def clear_database(self):\n",
    "        self.database = {}\n",
    "\n",
    "    def to_file(self, name):\n",
    "        with open(name + '.pkl', 'wb') as f:\n",
    "            pickle.dump(self.database, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def load_from_file(self, name):\n",
    "        with open(name+'.pkl', 'rb') as f:\n",
    "            self.database = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Database()  \n",
    "db.clear_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually index all the images in the memory database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBtrain: 0.00026154518127441406s\n"
     ]
    }
   ],
   "source": [
    "dbstart = time.time()\n",
    "for idx in range(len(train_images)):\n",
    "    db.add_to_db(train_images[idx], embeddings[idx])   \n",
    "dbend = time.time() \n",
    "dbtrain = dbend-dbstart \n",
    "print(f\"DBtrain: {dbtrain}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we use a function to calculate the l2 distance between a database image and the new incoming image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_face(img_embedding, db, threshold, debug = False): \n",
    "    curr_min = 9999  \n",
    "    curr_face = 'unknown' \n",
    "    \n",
    "    for face in db:  \n",
    "        for embedding in db[face]:\n",
    "            #L2 distance \n",
    "            dist = np.linalg.norm(np.subtract(embedding.cpu().detach().numpy(), img_embedding.cpu().detach().numpy()))  \n",
    "            if debug: \n",
    "                print(\"Dist for \" + face + \" > \"+ str(dist))\n",
    "            if(dist < curr_min):  \n",
    "                curr_min = dist \n",
    "                curr_face = face \n",
    "    \n",
    "    if curr_min*100 > threshold: \n",
    "        return curr_min, 'unknown'\n",
    "    else:  \n",
    "        return curr_min, curr_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4264202, '../ML_Training_Janine/dataset/dataset/lfw/cropped_split/train/Astrid_Eyzaguirre/augmented_dark_Astrid_Eyzaguirre_0001.jpg')\n",
      "0.010991334915161133 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "print(compare_face(embeddingtest, db.database, 420, False))\n",
    "end = time.time()\n",
    "seconds = end-start\n",
    "print(f\"{seconds} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex  \n",
    "import operator\n",
    "\n",
    "# Train annoy \n",
    "class Annoy: \n",
    "    \n",
    "    def __init__(self, vectorsize, name): \n",
    "        self.vectorsize = vectorsize  \n",
    "        self.name = name \n",
    "        self.mapping = {} \n",
    "        \n",
    "    def train(self, items, num_trees): \n",
    "        self.model = AnnoyIndex(self.vectorsize, 'angular') \n",
    "        for item in items: \n",
    "            name, vector = item  \n",
    "            \n",
    "            if name not in self.mapping:  \n",
    "                if(len(self.mapping.items()) > 0):\n",
    "                    highest_idx = max(self.mapping.items(), key=operator.itemgetter(1))[1]\n",
    "                else: \n",
    "                    highest_idx = 0\n",
    "                #print(f'Idx for {name}: {highest_idx}') \n",
    "                self.mapping[name] = highest_idx+1\n",
    "                \n",
    "            self.model.add_item(self.mapping[name], vector.detach().numpy().squeeze())  \n",
    "        \n",
    "        self.model.build(num_trees) \n",
    "        self.model.save(f'{self.name}.ann') \n",
    "    \n",
    "    def load(self): \n",
    "        self.model = AnnoyIndex(self.vectorsize, 'angular') \n",
    "        self.model.load(f'{self.name}.ann')  \n",
    "        \n",
    "    def predict(self): \n",
    "        pass \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "annoy = Annoy(512, 'performancetest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016185998916625977 s train annoy\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "annoy_train = [] \n",
    "for idx in range(len(train_images)):\n",
    "    annoy_train.append((idx, embeddings[idx].cpu())) \n",
    "annoy.train(annoy_train, 10) \n",
    "end = time.time()\n",
    "seconds_train_annoy = end-start\n",
    "print(f\"{seconds_train_annoy} s train annoy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1], [0.4264199733734131])\n",
      "0.0006687641143798828 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "prediction = annoy.model.get_nns_by_vector(embeddingtest.cpu().detach().numpy().squeeze(), 1, search_k=-1, include_distances=True)  \n",
    "print(prediction)\n",
    "end = time.time()\n",
    "seconds = end-start\n",
    "print(f\"{seconds} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "\n",
    "class PredictionSVM:\n",
    "    X = []\n",
    "    Y = []\n",
    "    Y_encoded = []\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    in_encoder = Normalizer(norm='l2')\n",
    "\n",
    "    model = SVC(kernel='linear', probability=True)\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, database):\n",
    "        for face in database:\n",
    "            for face_encode in database[face]:\n",
    "                self.X.append(self.in_encoder.transform(face_encode.detach().numpy()).flatten())\n",
    "                self.Y.append(face)\n",
    "\n",
    "        self.label_encoder.fit(self.Y)\n",
    "        self.Y_encoded = self.label_encoder.transform(self.Y)\n",
    "\n",
    "        self.model.fit(self.X, self.Y_encoded)\n",
    "        print(\"Model Trained\")\n",
    "\n",
    "    def predict(self, embedding):\n",
    "        embedding_numpy = embedding.detach().numpy()\n",
    "        faceclass = self.model.predict(embedding_numpy)\n",
    "        faceprobability = self.model.predict_proba(embedding_numpy)\n",
    "\n",
    "        class_index = faceclass[0]\n",
    "        class_label = self.label_encoder.inverse_transform(faceclass)[0]\n",
    "        \n",
    "        # TODO: Setup probabilities\n",
    "        print(faceprobability[0][0]*100)\n",
    "        return class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained\n",
      "0.9798634052276611 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()  \n",
    "prediction_model = PredictionSVM() \n",
    "prediction_model.train(db.database)\n",
    "end = time.time()  \n",
    "seconds_svm_train = end-start\n",
    "print(f\"{seconds_svm_train} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(embedding): \n",
    "    label = prediction_model.predict(embedding)\n",
    "\n",
    "    # Duplex prediction method \n",
    "    minl2distance = 9999 \n",
    "    for face in db.database[label]: \n",
    "        l2distance = embedding_model.l2(face, embedding)  \n",
    "        if l2distance < minl2distance:\n",
    "            minl2distance = l2distance \n",
    "\n",
    "    if minl2distance * 100 > 95:\n",
    "        label = \"unknown\" \n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7313113038167364\n",
      "../ML_Training_Janine/dataset/dataset/lfw/cropped_split/train/Astrid_Eyzaguirre/augmented_dark_Astrid_Eyzaguirre_0001.jpg\n",
      "0.003383636474609375 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time() \n",
    "print(prediction(embeddingtest.cpu()))\n",
    "end = time.time()  \n",
    "seconds_svm = end-start\n",
    "print(f\"{seconds_svm} s\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
